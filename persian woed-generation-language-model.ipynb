{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "107f58bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"G:/num.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b37ba211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "090394b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 478552 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b675f794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "نوید \r\n",
      "بهاره \r\n",
      "شیوا\r\n",
      "پری \r\n",
      "نیلوفر\r\n",
      "بهنام \r\n",
      "حمیده\r\n",
      "موژان \r\n",
      "مهسا \r\n",
      "دانیال\r\n",
      "مینا \r\n",
      "امیر \r\n",
      "حامد\r\n",
      "سهی \r\n",
      "سید مهدی\r\n",
      "شقایق \r\n",
      "سید آرمین \r\n",
      "نوشین \r\n",
      "پریسا\r\n",
      "فرامرز\r\n",
      "شیما سادات \r\n",
      "علیرضا\r\n",
      "غزل \r\n",
      "پروا\r\n",
      "میلاد \r\n",
      "مهسا\r\n",
      "مهدی\r\n",
      "عسل\r\n",
      "یاسمن \r\n",
      "الناز\r\n",
      "سیده سمانه\r\n",
      "یاسمین\r\n",
      "حورا\r\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e5de175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4643eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70f1d4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  '\\r':   1,\n",
      "  ' ' :   2,\n",
      "  \"'\" :   3,\n",
      "  '(' :   4,\n",
      "  ')' :   5,\n",
      "  '-' :   6,\n",
      "  '.' :   7,\n",
      "  '/' :   8,\n",
      "  '0' :   9,\n",
      "  '1' :  10,\n",
      "  '2' :  11,\n",
      "  '3' :  12,\n",
      "  '4' :  13,\n",
      "  '5' :  14,\n",
      "  '6' :  15,\n",
      "  '7' :  16,\n",
      "  '8' :  17,\n",
      "  '9' :  18,\n",
      "  ';' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40ed292a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'نوید \\r\\nبهاره ' ---- characters mapped to int ---- > [ 84  86 100  66   2   1   0  59  85  58  68  85   2]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "014057ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ن\n",
      "و\n",
      "ی\n",
      "د\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cbc1bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'نوید \\r\\nبهاره \\r\\nشیوا\\r\\nپری \\r\\nنیلوفر\\r\\nبهنام \\r\\nحمیده\\r\\nموژان \\r\\nمهسا \\r\\nدانیال\\r\\nمینا \\r\\nامیر \\r\\nحامد\\r\\nسهی \\r\\nسی'\n",
      "***************\n",
      "'د مهدی\\r\\nشقایق \\r\\nسید آرمین \\r\\nنوشین \\r\\nپریسا\\r\\nفرامرز\\r\\nشیما سادات \\r\\nعلیرضا\\r\\nغزل \\r\\nپروا\\r\\nمیلاد \\r\\nمهسا\\r\\nمهد'\n",
      "***************\n",
      "'ی\\r\\nعسل\\r\\nیاسمن \\r\\nالناز\\r\\nسیده سمانه\\r\\nیاسمین\\r\\nحورا\\r\\nآتوسا\\r\\nامید \\r\\nمریم\\r\\nرویا \\r\\nسجاد\\r\\nابراهیم \\r\\nسیده یاسم'\n",
      "***************\n",
      "'ن \\r\\nسپیده \\r\\nامیر حسین \\r\\nفضیلت \\r\\nسیدکیارش \\r\\nدنیا\\r\\nفریناز \\r\\nمحمدرضا\\r\\nریحانه\\r\\nمهستی سادات \\r\\nآیدا\\r\\nمیلاد\\r'\n",
      "***************\n",
      "'\\nفاطمه \\r\\nمحمد امین \\r\\nساناز \\r\\nحانیه \\r\\nمریم \\r\\nآرمان \\r\\nعرفان\\r\\nبهنام\\r\\nهلیا \\r\\nآرین \\r\\nملیکا\\r\\nنسرین\\r\\nپروین \\r'\n",
      "***************\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))\n",
    "    print(\"***\"*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14f835bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b75e212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'نوید \\r\\nبهاره \\r\\nشیوا\\r\\nپری \\r\\nنیلوفر\\r\\nبهنام \\r\\nحمیده\\r\\nموژان \\r\\nمهسا \\r\\nدانیال\\r\\nمینا \\r\\nامیر \\r\\nحامد\\r\\nسهی \\r\\nس'\n",
      "Target data: 'وید \\r\\nبهاره \\r\\nشیوا\\r\\nپری \\r\\nنیلوفر\\r\\nبهنام \\r\\nحمیده\\r\\nموژان \\r\\nمهسا \\r\\nدانیال\\r\\nمینا \\r\\nامیر \\r\\nحامد\\r\\nسهی \\r\\nسی'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "422fc605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 84 ('ن')\n",
      "  expected output: 86 ('و')\n",
      "Step    1\n",
      "  input: 86 ('و')\n",
      "  expected output: 100 ('ی')\n",
      "Step    2\n",
      "  input: 100 ('ی')\n",
      "  expected output: 66 ('د')\n",
      "Step    3\n",
      "  input: 66 ('د')\n",
      "  expected output: 2 (' ')\n",
      "Step    4\n",
      "  input: 2 (' ')\n",
      "  expected output: 1 ('\\r')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "867560ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(32, 100), dtype=tf.int32, name=None), TensorSpec(shape=(32, 100), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abd63d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 25\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9833ac",
   "metadata": {},
   "source": [
    "# LSTM  model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6adf1c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.LSTM(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)])\n",
    "     \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b2cb7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cb80bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 684ms/step\n",
      "(32, 100, 102) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model.predict(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf67fcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (32, None, 25)            2550      \n",
      "                                                                 \n",
      " lstm (LSTM)                 (32, None, 1024)          4300800   \n",
      "                                                                 \n",
      " dense (Dense)               (32, None, 102)           104550    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,407,900\n",
      "Trainable params: 4,407,900\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ba8da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eefef44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "model.compile(optimizer='adam',loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ddf9481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 55,  25,  58,  40, 101,  36, 101, 101,  45,  94,  58,  88,  93,\n",
       "        57,  98,  95,  49,  47,   8,  12,  88,  47,  62,  98,  65,  10,\n",
       "        70,  38,  90,  11,  31,  96,  38,  75,  58,  33,  87,  89,  16,\n",
       "        57,  88,  72,  36,  20,  22,  94,  28,  36,  71,  84,  78,  79,\n",
       "        49,  34,  16,  66,  22,  18,   1,  78, 101,  84,   3,  68,  58,\n",
       "        67,  11,  22,  16,   5,  16,  49,  59,  60,  69,  56,  57, 100,\n",
       "        82,  26,  48,  15,  45,  67,  95,  91,  61,  17,  86,  83,  38,\n",
       "        48,  27,  41,  38,  19,  59,  69,  41,  48], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec156e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'نوید \\r\\nبهاره \\r\\nشیوا\\r\\nپری \\r\\nنیلوفر\\r\\nبهنام \\r\\nحمیده\\r\\nموژان \\r\\nمهسا \\r\\nدانیال\\r\\nمینا \\r\\nامیر \\r\\nحامد\\r\\nسهی \\r\\nس'\n",
      "\n",
      "Next Char Predictions: \n",
      " \"أLاn\\u200ci\\u200c\\u200ctٔاًّئکپ\\xa0v/3ًvثکخ1سlَ2dچlظاfيٍ7ئًصi?DٔUiشنـف\\xa0g7دD9\\rـ\\u200cن'راذ2D7)7\\xa0بةزؤئیلNy6tذپُت8ومlyOol;بزoy\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3d92d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60182f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e712ade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f24baff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "148/148 [==============================] - 443s 3s/step - loss: 3.0948\n",
      "Epoch 2/40\n",
      "148/148 [==============================] - 400s 3s/step - loss: 2.6207\n",
      "Epoch 3/40\n",
      "148/148 [==============================] - 396s 3s/step - loss: 2.4838\n",
      "Epoch 4/40\n",
      "148/148 [==============================] - 405s 3s/step - loss: 2.4283\n",
      "Epoch 5/40\n",
      "148/148 [==============================] - 420s 3s/step - loss: 2.3826\n",
      "Epoch 6/40\n",
      "148/148 [==============================] - 419s 3s/step - loss: 2.3373\n",
      "Epoch 7/40\n",
      "148/148 [==============================] - 418s 3s/step - loss: 2.2849\n",
      "Epoch 8/40\n",
      "148/148 [==============================] - 414s 3s/step - loss: 2.2286\n",
      "Epoch 9/40\n",
      "148/148 [==============================] - 406s 3s/step - loss: 2.1649\n",
      "Epoch 10/40\n",
      "148/148 [==============================] - 490s 3s/step - loss: 2.0979\n",
      "Epoch 11/40\n",
      "148/148 [==============================] - 447s 3s/step - loss: 2.0302\n",
      "Epoch 12/40\n",
      "148/148 [==============================] - 428s 3s/step - loss: 1.9636\n",
      "Epoch 13/40\n",
      "148/148 [==============================] - 419s 3s/step - loss: 1.8997\n",
      "Epoch 14/40\n",
      "148/148 [==============================] - 419s 3s/step - loss: 1.8418\n",
      "Epoch 15/40\n",
      "148/148 [==============================] - 408s 3s/step - loss: 1.7880\n",
      "Epoch 16/40\n",
      "148/148 [==============================] - 387s 3s/step - loss: 1.7347\n",
      "Epoch 17/40\n",
      "148/148 [==============================] - 437s 3s/step - loss: 1.6855\n",
      "Epoch 18/40\n",
      "148/148 [==============================] - 436s 3s/step - loss: 1.6386\n",
      "Epoch 19/40\n",
      "148/148 [==============================] - 432s 3s/step - loss: 1.5887\n",
      "Epoch 20/40\n",
      "148/148 [==============================] - 432s 3s/step - loss: 1.5421\n",
      "Epoch 21/40\n",
      "148/148 [==============================] - 403s 3s/step - loss: 1.4897\n",
      "Epoch 22/40\n",
      "148/148 [==============================] - 406s 3s/step - loss: 1.4401\n",
      "Epoch 23/40\n",
      "148/148 [==============================] - 403s 3s/step - loss: 1.3886\n",
      "Epoch 24/40\n",
      "148/148 [==============================] - 426s 3s/step - loss: 1.3523\n",
      "Epoch 25/40\n",
      "148/148 [==============================] - 470s 3s/step - loss: 1.3154\n",
      "Epoch 26/40\n",
      "148/148 [==============================] - 474s 3s/step - loss: 1.2521\n",
      "Epoch 27/40\n",
      "148/148 [==============================] - 443s 3s/step - loss: 1.2008\n",
      "Epoch 28/40\n",
      "148/148 [==============================] - 476s 3s/step - loss: 1.1587\n",
      "Epoch 29/40\n",
      "148/148 [==============================] - 509s 3s/step - loss: 1.1001\n",
      "Epoch 30/40\n",
      "148/148 [==============================] - 486s 3s/step - loss: 1.0440\n",
      "Epoch 31/40\n",
      "148/148 [==============================] - 499s 3s/step - loss: 1.0169\n",
      "Epoch 32/40\n",
      "148/148 [==============================] - 473s 3s/step - loss: 0.9573\n",
      "Epoch 33/40\n",
      "148/148 [==============================] - 475s 3s/step - loss: 0.9116\n",
      "Epoch 34/40\n",
      "148/148 [==============================] - 491s 3s/step - loss: 0.8712\n",
      "Epoch 35/40\n",
      "148/148 [==============================] - 503s 3s/step - loss: 0.8376\n",
      "Epoch 36/40\n",
      "148/148 [==============================] - 474s 3s/step - loss: 0.7790\n",
      "Epoch 37/40\n",
      "148/148 [==============================] - 465s 3s/step - loss: 0.7380\n",
      "Epoch 38/40\n",
      "148/148 [==============================] - 451s 3s/step - loss: 0.7030\n",
      "Epoch 39/40\n",
      "148/148 [==============================] - 441s 3s/step - loss: 0.6644\n",
      "Epoch 40/40\n",
      "148/148 [==============================] - 433s 3s/step - loss: 0.6124\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "313a6c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\ckpt_40'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f791279",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "758c3b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the word returned by the model\n",
    "      predictions = predictions \n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5aeb2d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "امیری\r\n",
      "پائيزی\r\n",
      "میر مددحقيقي\r\n",
      "قشوادی پورجهان \r\n",
      "فرهودی\r\n",
      "عارفه نوری\r\n",
      "شهامی سروستاني\r\n",
      "زینالي فر\r\n",
      "مطلبی پور\r\n",
      "رجبي فچا\r\n",
      "بنائی شوطهري\r\n",
      "مدنی\r\n",
      "عادلخانی\r\n",
      "کرمی رباب ارام\r\n",
      "ژرگر بامله\r\n",
      "دن آرا\r\n",
      "پوریافي \r\n",
      "روز بهنا \r\n",
      "عبادی\r\n",
      "فاضل اردبيلي\r\n",
      "کاشگرور مهری\r\n",
      "ابندي مطاع \r\n",
      "وادگاني\r\n",
      "مروت قاسمی \r\n",
      "مهدی زاده \r\n",
      "مشاوریان تهوهه\r\n",
      "زنانی\r\n",
      "امیدی گرماني \r\n",
      "حسینعلی وانجان\r\n",
      "اوجیانی\r\n",
      "بازین دلد\r\n",
      "محمدی خورده\r\n",
      "یاراحمدی\r\n",
      "كیانی فرد\r\n",
      "خامران زماني\r\n",
      "فجود لوله لو\r\n",
      "عبادفرد\r\n",
      "وادظافري\r\n",
      "اخوان مهر\r\n",
      "ملیحي\r\n",
      "عنیدی مقدم\r\n",
      "بهشتي زرنقی\r\n",
      "رهیدی دريزي\r\n",
      "ميرشرامیان\r\n",
      "جاهدمنش\r\n",
      "فردخي \r\n",
      "قدمدی تبریز\r\n",
      "گل گین اوغلی\r\n",
      "ناصری پور\r\n",
      "امیدی طرقی\r\n",
      "میرفخرایی شريفی\r\n",
      "میرباقري فير آبادی\r\n",
      "باحجب فردوس\r\n",
      "عرب زاده ارونقي زاده\r\n",
      "کاوه محمدی\r\n",
      "اسماعیل خان قصري\r\n",
      "گودرزيان\r\n",
      "رمضان زاده طوسه\r\n",
      "محجر قنبري\r\n",
      "مامانی \r\n",
      "اردلی جاد\r\n",
      "امامعلی زاده\r\n",
      "اطهري\r\n",
      "پژوعلی\r\n",
      "پورحیدری\r\n",
      "اندوهی\r\n",
      "موسوی مشکوی\r\n",
      "خمیعیان\r\n",
      "جنوبدی\r\n",
      "عمادورد\r\n",
      "نوسخيابانیان\r\n",
      "رجبيعراق محمدی\r\n",
      "جوانمرد اول\r\n",
      "عبودی حسامی\r\n",
      "ماهوتچیان\r\n",
      "خمیاي\r\n",
      "عزيززاده شريفي\r\n",
      "ميرزاي پیشگاه آبادو\r\n",
      "امام لاعی\r\n",
      " شهسواری گودرزابی\r\n",
      "حلاج\r\n",
      "جمالباز \r\n",
      "ولدي\r\n",
      "علائيان\r\n",
      "امیرحسنی\r\n",
      "زم\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"امیر\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5962c3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
